{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Travail B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import accuracy_score\n",
    "%config InteractiveShell.ast_node_interactivity='last_expr_or_assign'\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBScan\n",
    "> Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 203 with at least 1 missing value, out of 435 rows\n"
     ]
    }
   ],
   "source": [
    "path_vote = './vote.csv'\n",
    "df_vote = pd.read_csv(path_vote)\n",
    "mapping = {'y': 1, 'n':0, '?':None, 'republican':1, 'democrat':0}\n",
    "df_vote = df_vote.transform(lambda x: x.apply(lambda c: mapping[c]))\n",
    "incomplete_rows = df_vote.isnull().any(axis=1).sum()\n",
    "print(f'There are {incomplete_rows} with at least 1 missing value, out of {len(df_vote)} rows')\n",
    "df_vote.dropna(inplace=True)\n",
    "columns_vote = df_vote.columns\n",
    "X_vote, y_vote = df_vote[columns_vote[:-1]], df_vote[columns_vote[-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> L'accuracy est calculée uniquement pour les données qui ont un cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(y_true, y_pred):\n",
    "    \"\"\" \n",
    "    Get accuracy for instances that have a cluster\n",
    "    \"\"\"\n",
    "    return max(s:=(sum(p:=[x == y for x,y in zip(y_true, y_pred) if y in [0,1]])/len(p)), 1-s)\n",
    "\n",
    "def get_global_accuracy(y_true, y_pred):\n",
    "    \"\"\" \n",
    "    Get accuracy by considering that an instance without cluster is wrongly classified\n",
    "    \"\"\"\n",
    "    return max(sum(x == y for x,y in zip(y_true, y_pred) if y in [0,1]), sum(x != y for x,y in zip(y_true, y_pred) if y in [0,1])) / len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not classified: [61, 71, 79, 83]\n",
      "accuracies = [0.9813664596273292, 0.9813664596273292, 0.9869281045751634, 0.9865771812080537]\n",
      "global accuracies = [0.6810344827586207, 0.6810344827586207, 0.6508620689655172, 0.6336206896551724]\n",
      "Nombre de clusters (en comptant le cluster des instances sans cluster): [8, 3, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "global_accuracies = []\n",
    "not_classified = []\n",
    "number_points = [2,3,4,6]\n",
    "n_clusters = []\n",
    "for min_points in number_points:\n",
    "    clustering = DBSCAN(eps=1.02, min_samples=min_points).fit(X_vote.values)\n",
    "    accuracies.append(get_accuracy(y_vote, clustering.labels_))\n",
    "    global_accuracies.append(get_global_accuracy(y_vote, clustering.labels_))\n",
    "    not_classified.append(sum(y == -1 for y in clustering.labels_))\n",
    "    n_clusters.append(len(np.unique(clustering.labels_)))\n",
    "print(f'not classified: {not_classified}')\n",
    "print(f'accuracies = {accuracies}')\n",
    "print(f'global accuracies = {global_accuracies}')\n",
    "print(f'Nombre de clusters (en comptant le cluster des instances sans cluster): {n_clusters}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def argmax(list):\n",
    "    return max((x,i) for i,x in enumerate(list))[1]\n",
    "best_min_points = number_points[argmax(global_accuracies)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Calcul du meilleur modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.6810344827586207\n",
      "confusion matrix: \n",
      "[[76  3]\n",
      " [ 0 82]]\n",
      "Number not classified: 71\n"
     ]
    }
   ],
   "source": [
    "best_db_scan = DBSCAN(eps=1.02, min_samples=best_min_points).fit(X_vote.values)\n",
    "best_acc = get_global_accuracy(y_vote, best_db_scan.labels_)\n",
    "labels_true_pred = [(t,p)for t,p in zip(y_vote, best_db_scan.labels_) if p != -1]\n",
    "cm = confusion_matrix([x[0] for x in labels_true_pred], [abs(x[1]-1) for x in labels_true_pred])\n",
    "print(f'accuracy: {best_acc}\\nconfusion matrix: \\n{cm}')\n",
    "print(f'Number not classified: {sum(x == -1 for x in best_db_scan.labels_)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> On a donc 0 Democrat mal placé et 1 Republican mal placé, pour ceux qui ont été classés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Recherche de la meilleure valeur de epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not classified: [71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71]\n",
      "accuracies = [0.9813664596273292, 0.9813664596273292, 0.9813664596273292, 0.9813664596273292, 0.9813664596273292, 0.9813664596273292, 0.9813664596273292, 0.9813664596273292, 0.9813664596273292, 0.9813664596273292, 0.9813664596273292, 0.9813664596273292, 0.9813664596273292, 0.9813664596273292, 0.9813664596273292, 0.9813664596273292, 0.9813664596273292, 0.9813664596273292, 0.9813664596273292, 0.9813664596273292, 0.9813664596273292]\n",
      "global accuracies = [0.6810344827586207, 0.6810344827586207, 0.6810344827586207, 0.6810344827586207, 0.6810344827586207, 0.6810344827586207, 0.6810344827586207, 0.6810344827586207, 0.6810344827586207, 0.6810344827586207, 0.6810344827586207, 0.6810344827586207, 0.6810344827586207, 0.6810344827586207, 0.6810344827586207, 0.6810344827586207, 0.6810344827586207, 0.6810344827586207, 0.6810344827586207, 0.6810344827586207, 0.6810344827586207]\n",
      "n_clusters = 3\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "global_accuracies = []\n",
    "not_classified = []\n",
    "epsilons = np.arange(1.00, 1.02, 0.001)\n",
    "n_clusters = 0\n",
    "for eps in epsilons:\n",
    "    clustering = DBSCAN(eps=eps, min_samples=best_min_points).fit(X_vote.values)\n",
    "    accuracies.append(get_accuracy(y_vote, clustering.labels_))\n",
    "    global_accuracies.append(get_global_accuracy(y_vote, clustering.labels_))\n",
    "    not_classified.append(sum(y == -1 for y in clustering.labels_))\n",
    "    n_clusters = len(np.unique(clustering.labels_))\n",
    "print(f'not classified: {not_classified}')\n",
    "print(f'accuracies = {accuracies}')\n",
    "print(f'global accuracies = {global_accuracies}')\n",
    "print(f'n_clusters = {n_clusters}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Dans ce cas, changer epsilon n'a pas d'impact sur le résultat. <br>\n",
    "> Il semble que les paramètres n'ont pas le même impact sur Weka et python. <br>\n",
    "> Une grid-search ne peut être faite car une fonction de cout n'est pas défini pour le clustering dans scikit-learn (cela pourrait être implémenté à la main dans notre exemple, et c'est plus ou moins ce qu'on fait avec les 2 boucles sur epsilon et min_sample, qui sont cependant séparées). <br>\n",
    "\n",
    "> J'ai changé les valeurs de epsilons pour avoir des résultats différents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not classified: [148, 148, 148, 148, 148, 148, 148, 148, 148, 71, 71, 71, 71, 71, 21, 21, 21, 1, 1]\n",
      "accuracies = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9813664596273292, 0.9813664596273292, 0.9813664596273292, 0.9813664596273292, 0.9813664596273292, 0.927536231884058, 0.927536231884058, 0.927536231884058, 0.5324675324675324, 0.5324675324675324]\n",
      "global accuracies = [0.05172413793103448, 0.05172413793103448, 0.05172413793103448, 0.05172413793103448, 0.05172413793103448, 0.05172413793103448, 0.05172413793103448, 0.05172413793103448, 0.05172413793103448, 0.6810344827586207, 0.6810344827586207, 0.6810344827586207, 0.6810344827586207, 0.6810344827586207, 0.8275862068965517, 0.8275862068965517, 0.8275862068965517, 0.5301724137931034, 0.5301724137931034]\n",
      "n_clusters = [20, 20, 20, 20, 20, 20, 20, 20, 20, 3, 3, 3, 3, 3, 4, 4, 4, 2, 2]\n",
      "best epsilon: 1.7000000000000002\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "global_accuracies = []\n",
    "not_classified = []\n",
    "epsilons = np.arange(.1, 2.00, 0.1)\n",
    "n_clusters = []\n",
    "for eps in epsilons:\n",
    "    clustering = DBSCAN(eps=eps, min_samples=best_min_points).fit(X_vote.values)\n",
    "    accuracies.append(get_accuracy(y_vote, clustering.labels_))\n",
    "    global_accuracies.append(get_global_accuracy(y_vote, clustering.labels_))\n",
    "    not_classified.append(sum(y == -1 for y in clustering.labels_))\n",
    "    n_clusters.append(len(np.unique(clustering.labels_)))\n",
    "print(f'not classified: {not_classified}')\n",
    "print(f'accuracies = {accuracies}')\n",
    "print(f'global accuracies = {global_accuracies}')\n",
    "print(f'n_clusters = {n_clusters}')\n",
    "best_epsilon = epsilons[argmax(global_accuracies)]\n",
    "print(f'best epsilon: {best_epsilon}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Calcul du meilleur modèle avec best epsilon et best_min_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8275862068965517\n",
      "confusion matrix: \n",
      "[[95 15]\n",
      " [ 4 97]]\n",
      "Number not classified: 21\n"
     ]
    }
   ],
   "source": [
    "best_db_scan = DBSCAN(eps=best_epsilon, min_samples=best_min_points).fit(X_vote.values)\n",
    "best_acc = get_global_accuracy(y_vote, best_db_scan.labels_)\n",
    "labels_true_pred = [(t,p)for t,p in zip(y_vote, best_db_scan.labels_) if p != -1]\n",
    "cm = confusion_matrix([x[0] for x in labels_true_pred], [abs(x[1]-1) for x in labels_true_pred])\n",
    "print(f'accuracy: {best_acc}\\nconfusion matrix: \\n{cm}')\n",
    "print(f'Number not classified: {sum(x == -1 for x in best_db_scan.labels_)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Ce dernier modèle est meilleur que le précédent car moins de données sont non classifiées, et l'accuracy globale est meilleure. <br>\n",
    ">> Democrat classifié mal classifié: 4 <br>\n",
    ">> Republican classifié mal classifié: 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Méthode probabiliste EM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_weather = './weather.nominalToBinary.csv'\n",
    "df_weather = pd.read_csv(path_weather)\n",
    "df_weather['play'] = df_weather['play'].apply(lambda x: 0 if x == 'no' else 1)\n",
    "weather_columns = df_weather.columns\n",
    "X, y = df_weather[weather_columns[:-1]], df_weather[weather_columns[-1]]\n",
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Calcul du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.57\n",
      "confusion matrix: \n",
      "[[3 2]\n",
      " [4 5]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.60      0.50         5\n",
      "           1       0.71      0.56      0.63         9\n",
      "\n",
      "    accuracy                           0.57        14\n",
      "   macro avg       0.57      0.58      0.56        14\n",
      "weighted avg       0.61      0.57      0.58        14\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gm = GaussianMixture(n_components=2, random_state=0).fit(X)\n",
    "predictions = gm.predict(X)\n",
    "def print_metrics(y_train, y_pred):\n",
    "    accuracy = sum(y_pred == y_train.to_numpy())/len(y_pred)\n",
    "    print(f'accuracy = {accuracy:.2f}')\n",
    "    print(f'confusion matrix: \\n{confusion_matrix(y_train, y_pred)}')\n",
    "    print(classification_report(y_train, y_pred))\n",
    "print_metrics(y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Variation de la seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5714285714285714,\n",
       " 0.5714285714285714,\n",
       " 0.5714285714285714,\n",
       " 0.5714285714285714,\n",
       " 0.5714285714285714]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies = []\n",
    "seeds = [0,5,10,20,100]\n",
    "for s in seeds:\n",
    "    gm = GaussianMixture(n_components=2, random_state=0).fit(X)\n",
    "    predictions = gm.predict(X)\n",
    "    accuracies.append(accuracy_score(y, predictions))\n",
    "accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Il n'y a pas de différence dans les résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 2],\n",
       "       [4, 5]])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> no mal placés: 2 <br>\n",
    "> yes mal placés: 4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
