{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BE UNet \n",
    "> Victor Ludvig, February 2024 <br>\n",
    "> Prof. L. Chen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.nn import BCELoss\n",
    "from torch.optim import Adam\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor, Resize, Compose\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import matplotlib.pyplot as plt \n",
    "from torch.optim.lr_scheduler import MultiplicativeLR\n",
    "PATH_ROOT = './competition_data/competition_data/train'\n",
    "PATH_IMAGES = os.path.join(PATH_ROOT, 'images')\n",
    "PATH_MASKS = os.path.join(PATH_ROOT, 'masks')\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementatin of UNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computation of the transpose convolution parameters\n",
    "> The transpose convolution layers upsample the image by doubling its width and height. <br>\n",
    "> The downsampling is achieved using a MaxPool2D layer in the encoder with kernel k=2, stride s=2 and padding p=0. <br>\n",
    "> To upsample with PyTorch, we can simply use the same parameters for the TransposeConvolution: k=2, s=2, p=0. <br>\n",
    "> The stride s=2 adds one 0 between each pixel, so the convolution with a kernel of size 2 will effectively double the image width and height. <br>\n",
    "\n",
    "> This result can be checked using the formula from the [Pytorch website](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html):\n",
    "$\\begin{equation} H_{out}​=(H_{in}​−1)×stride−2×padding+dilation×(kernel\\_size−1)+output\\_padding+1 \\end{equation}$\n",
    "\n",
    "> With : \n",
    ">> $H_{out}​=2H_{in}$ <br>\n",
    ">> $stride=2$ <br>\n",
    ">> $dilation = 1$ <br>\n",
    ">> $kernel\\_size = 2$ <br>\n",
    ">> $output\\_padding = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        # Super constructor\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding='same'),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, padding='same'), \n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding='same'),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding='same'),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding='same'),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding='same'),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding='same'),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding='same'),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding='same'),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding='same'),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Decoder \n",
    "        \n",
    "        self.transpose_conv1 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv6 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=128, kernel_size=3, padding='same'),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding='same'),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.transpose_conv2 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv7 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, padding='same'),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding='same'),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.transpose_conv3 = nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv8 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, padding='same'),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding='same'),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.transpose_conv4 = nn.ConvTranspose2d(in_channels=32, out_channels=16, kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv9 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=16, kernel_size=3, padding='same'),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, padding='same'),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.conv10 = nn.Conv2d(in_channels=16, out_channels=1, kernel_size=1)\n",
    "        \n",
    "\n",
    "    def forward(self, images):\n",
    "        # Encoder\n",
    "\n",
    "        c1 = self.conv1(images)\n",
    "        p1 = self.pool1(c1)\n",
    "\n",
    "        c2 = self.conv2(p1)\n",
    "        p2 = self.pool2(c2)\n",
    "\n",
    "        c3 = self.conv3(p2)\n",
    "        p3 = self.pool3(c3)\n",
    "\n",
    "        c4 = self.conv4(p3)\n",
    "        p4 = self.pool4(c4)\n",
    "\n",
    "        c5 = self.conv5(p4)\n",
    "\n",
    "        # Decoder \n",
    "\n",
    "        u6 = self.transpose_conv1(c5)\n",
    "        u6 = torch.cat((u6, c4), dim=1) # first dim is batch dimension, second (1) is the channel\n",
    "\n",
    "        c6 = self.conv6(u6)\n",
    "        u7 = self.transpose_conv2(c6)\n",
    "        u7 = torch.cat((u7, c3), dim=1)\n",
    "\n",
    "        c7 = self.conv7(u7)\n",
    "        u8 = self.transpose_conv3(c7)\n",
    "        u8 = torch.cat((u8, c2), dim=1)\n",
    "\n",
    "        c8 = self.conv8(u8)\n",
    "        u9 = self.transpose_conv4(c8)\n",
    "        u9 = torch.cat((u9, c1), dim=1)\n",
    "\n",
    "        c9 = self.conv9(u9)\n",
    "        \n",
    "        return torch.sigmoid(self.conv10(c9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNet = UNet()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "UNet.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The learning rate will be divided by 10 when 5 consecutive epochs don't improve the validation loss <br>\n",
    "> It works very well, and the threshold could be increased to 15 epochs without improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Creation of custom Dataset class to load data <br>\n",
    "> Some images are loaded by Pillow with an alpha channel with all values to 255. <br>\n",
    "The alpha channel is discarded using im = im.convert(\"RGB\")<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "\n",
    "    def __init__(self, list_images, list_masks, transform=None):\n",
    "        self.list_images = list_images\n",
    "        self.list_masks = list_masks\n",
    "        \n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.list_images[idx]\n",
    "        with Image.open(img_path) as im:\n",
    "            im = im.convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                im = self.transform(im)\n",
    "\n",
    "        mask_path = self.list_masks[idx]\n",
    "        with Image.open(mask_path) as im_mask:\n",
    "            im_mask = im_mask.convert('L')\n",
    "            if self.transform:\n",
    "                im_mask = self.transform(im_mask)\n",
    "\n",
    "        return im, im_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Divide train/val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The train images are divided into train/val images <br>\n",
    "> 20% of the images are used as validation images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_images = [os.path.join(PATH_IMAGES,x) for x in os.listdir(PATH_IMAGES)]\n",
    "list_masks = [os.path.join(PATH_MASKS,x) for x in os.listdir(PATH_MASKS)]\n",
    "path_train_images, path_val_images, path_train_masks, path_val_masks = train_test_split(list_images, list_masks, test_size=0.2, random_state=0)\n",
    "\n",
    "assert len(path_train_images) == len(path_train_masks)\n",
    "print(f'Number of training images: {len(path_train_images)}')\n",
    "print(f'Number of validation images: {len(path_val_images)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyperparameters(model):\n",
    "    \"\"\"\"Get hyperparameters\"\"\"\n",
    "    N_EPOCHS = 200\n",
    "    LR = 0.0001\n",
    "    BATCH_SIZE = 32\n",
    "    optimizer = Adam(model.parameters(), lr=LR)\n",
    "    scheduler = MultiplicativeLR(optimizer, lr_lambda=lambda epoch: .1)\n",
    "    criterion = BCELoss()\n",
    "    return N_EPOCHS, LR, BATCH_SIZE, optimizer, scheduler, criterion\n",
    "\n",
    "def get_data_loader(path_train_images, path_train_masks, path_val_images, path_val_masks, BATCH_SIZE):\n",
    "    \"\"\" Get data loader. \"\"\"\n",
    "    train_dataset = ImageDataset(path_train_images, path_train_masks, Compose([ToTensor(), Resize((128,128))]))\n",
    "    val_dataset = ImageDataset(path_val_images, path_val_masks, Compose([ToTensor(), Resize((128,128))]))\n",
    "\n",
    "    train_data_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                            batch_size=BATCH_SIZE,\n",
    "                                            shuffle=True,\n",
    "                                            num_workers=0)\n",
    "\n",
    "    val_data_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                            batch_size=BATCH_SIZE,\n",
    "                                            shuffle=True,\n",
    "                                            num_workers=0)\n",
    "\n",
    "    dataloaders = {'train': train_data_loader,\n",
    "                'val': val_data_loader}\n",
    "    \n",
    "    dataset_sizes = {'train': len(path_train_images),\n",
    "                     'val': len(path_val_images)}\n",
    "    \n",
    "    return dataloaders, dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS, LR, BATCH_SIZE, optimizer, scheduler, criterion = get_hyperparameters(UNet)\n",
    "dataloaders, dataset_sizes = get_data_loader(path_train_images, path_train_masks, path_val_images, path_val_masks, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, dataloaders, N_EPOCHS, scheduler):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = 1e10\n",
    "    best_model_weights = copy.deepcopy(model.state_dict())\n",
    "    # threshold of consecutive epochs without improvement to know when to update the learning rate\n",
    "    threshold_lr = 0\n",
    "    threshold_stop = 0\n",
    "\n",
    "    for epoch in tqdm(range(N_EPOCHS)):\n",
    "        print(f'\\n\\nEpoch {epoch}/{N_EPOCHS}')\n",
    "        print('-'*10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            print(f'Phase: {phase}')\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else: \n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "            for inputs, masks in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                masks = masks.to(device)\n",
    "\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, masks)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step() \n",
    "\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            average_epoch_loss = running_loss / dataset_sizes[phase]\n",
    "\n",
    "            print(f'Average {phase} loss: {average_epoch_loss:.4f}')\n",
    "\n",
    "            if phase == 'val':\n",
    "                val_losses.append(average_epoch_loss)\n",
    "                if average_epoch_loss < best_val_loss:\n",
    "                    best_val_loss = average_epoch_loss\n",
    "                    best_model_weights = copy.deepcopy(model.state_dict())\n",
    "                    threshold_lr = 0\n",
    "                    threshold_stop = 0\n",
    "                else:\n",
    "                    threshold_lr += 1\n",
    "                    threshold_stop += 1 \n",
    "                if threshold_lr >= 10:\n",
    "                    scheduler.step()\n",
    "                    threshold_lr = 0\n",
    "                    threshold_stop += 1  \n",
    "                print(f'threshold_lr: {threshold_lr}')\n",
    "            else:\n",
    "                train_losses.append(average_epoch_loss)\n",
    "\n",
    "            # early stopping in 20 epochs without improvement\n",
    "            if threshold_stop >= 20:\n",
    "                return model, train_losses, val_losses  \n",
    "\n",
    "            \n",
    "    model.load_state_dict(best_model_weights)\n",
    "    return model, train_losses, val_losses  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_unet, train_losses, val_losses = train(UNet, optimizer, criterion, dataloaders, N_EPOCHS, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('./model'):\n",
    "    os.mkdir('model')\n",
    "torch.save(trained_unet.state_dict(), './model/trained_unet_1.pt')\n",
    "\n",
    "if not os.path.isdir('./figures'):\n",
    "    os.mkdir('figures')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The UNet model trains well over 200 epochs, without overfitting. <br>\n",
    "> The training could have gone further. <br>\n",
    "> However we see that at the end the incremental improvements are minimal. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(N_epochs, train_losses, val_losses, title='test.png'):\n",
    "    epochs = range(1, N_epochs)  # Assuming 200 epochs\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))  # Adjust figure size as needed\n",
    "    plt.plot(epochs, val_losses, label='Validation Loss')\n",
    "    plt.plot(epochs, train_losses, label='Training Loss')\n",
    "    plt.title('Training and Validation Loss over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()  # Adjust layout\n",
    "    plt.savefig(f'./figures/{title}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training(N_EPOCHS+1, train_losses, val_losses, 'training_1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![figures/unet_training.png](figures/unet_training.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Inference\n",
    "> On training set first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_unet = UNet()\n",
    "trained_unet.load_state_dict(state_dict=torch.load('./model/trained_unet_1.pt'))\n",
    "trained_unet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(n, inputs, masks_rgb, outputs, img_name=''):\n",
    "    \"\"\" Plot ground-truth, salt, predictions, binary predictions side by side, like in the pdf. \n",
    "    n: number of ground-truth images to plot\n",
    "    \"\"\"\n",
    "    outputs_binary = torch.where(outputs > .5, 1., 0.).repeat(1, 3, 1, 1)\n",
    "    outputs = outputs.repeat(1, 3, 1, 1)\n",
    "\n",
    "    # Concatenate the images along the width dimension to create a single tensor\n",
    "    combined_images = torch.cat([inputs[0:n,:,:,:], masks_rgb[0:n,:,:,:], outputs[0:n,:,:,:], outputs_binary[0:n,:,:,:]], dim=3)\n",
    "\n",
    "    # Make a grid with the combined images\n",
    "    grid_image = torchvision.utils.make_grid(combined_images, nrow=1, padding=10, normalize=True)\n",
    "\n",
    "    # Convert the grid tensor to a numpy array for visualization\n",
    "    grid_image_np = grid_image.permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "    # Display the grid of images\n",
    "    plt.figure(figsize=(10, 15))\n",
    "    plt.imshow(grid_image_np)\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    plt.savefig(f'figures/{img_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(model, dataloader, name_img):\n",
    "    inputs, masks = next(iter(dataloader))\n",
    "    masks = masks.to(device)\n",
    "    masks_rgb = masks.repeat(1, 3, 1, 1)\n",
    "    inputs = inputs.to(device)\n",
    "    outputs = model(inputs)\n",
    "    plot_predictions(5, inputs, masks_rgb, outputs, name_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Results are very good on these training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions(trained_unet, dataloaders['val'], 'inference_validation.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![figures/inference_validation.png](figures/inference_validation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Results are also pretty good on the validation samples, although a bit less accuracte on the last image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5- Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1) The 2x2 MaxPool kernels can be replaced with 2x2 Conv2D kernel with padding 1 and stride 2. The resulting size will still be divided by two since the MaxPool kernel has the same effect as the Conv2D kernel on the size. <br>\n",
    "It can be checked with the formula from [Pytorch website](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html): <br>\n",
    "$\\begin{equation} H_{out}​=\\frac{H_{in}+2×padding+dilation×(kernel\\_size−1)-1}{stride}+1\\end{equation}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding='same'),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding='same'),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(f'Conv2 forward, x.shape = {x.shape}')\n",
    "        return self.conv(x)\n",
    "    \n",
    "class ConvUnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        # Super constructor\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv2(in_channels=3, out_channels=16)\n",
    "        self.pool_conv1 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = Conv2(in_channels=16, out_channels=32)\n",
    "        self.pool_conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv3 = Conv2(in_channels=32, out_channels=64)\n",
    "        self.pool_conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv4 = Conv2(in_channels=64, out_channels=128)\n",
    "        self.pool_conv4 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv5 = Conv2(in_channels=128, out_channels=256)\n",
    "\n",
    "        # Decoder \n",
    "        \n",
    "        self.transpose_conv1 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv6 = Conv2(in_channels=256, out_channels=128)\n",
    "        self.transpose_conv2 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv7 = Conv2(in_channels=128, out_channels=64)\n",
    "        self.transpose_conv3 = nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv8 = Conv2(in_channels=64, out_channels=32)\n",
    "        self.transpose_conv4 = nn.ConvTranspose2d(in_channels=32, out_channels=16, kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv9 = Conv2(in_channels=32, out_channels=16)\n",
    "\n",
    "        self.conv10 = nn.Conv2d(in_channels=16, out_channels=1, kernel_size=1)\n",
    "        \n",
    "\n",
    "    def forward(self, images):\n",
    "        # Encoder\n",
    "\n",
    "        # print(f'images.shape = {images.shape}')\n",
    "        c1 = self.conv1(images)\n",
    "        # print(f'c1.shape = {c1.shape}')\n",
    "        p1 = self.pool_conv1(c1)\n",
    "\n",
    "        c2 = self.conv2(p1)\n",
    "        # print(f'c2.shape = {c1.shape}')\n",
    "        p2 = self.pool_conv2(c2)\n",
    "\n",
    "        c3 = self.conv3(p2)\n",
    "        p3 = self.pool_conv3(c3)\n",
    "\n",
    "        c4 = self.conv4(p3)\n",
    "        p4 = self.pool_conv4(c4)\n",
    "\n",
    "        c5 = self.conv5(p4)\n",
    "\n",
    "        # Decoder \n",
    "\n",
    "        u6 = self.transpose_conv1(c5)\n",
    "        u6 = torch.cat((u6, c4), dim=1) # first dim is batch dimension, second (1) is the channel\n",
    "\n",
    "        c6 = self.conv6(u6)\n",
    "        u7 = self.transpose_conv2(c6)\n",
    "        u7 = torch.cat((u7, c3), dim=1)\n",
    "\n",
    "        c7 = self.conv7(u7)\n",
    "        u8 = self.transpose_conv3(c7)\n",
    "        u8 = torch.cat((u8, c2), dim=1)\n",
    "\n",
    "        c8 = self.conv8(u8)\n",
    "        u9 = self.transpose_conv4(c8)\n",
    "        u9 = torch.cat((u9, c1), dim=1)\n",
    "\n",
    "        c9 = self.conv9(u9)\n",
    "        \n",
    "        return torch.sigmoid(self.conv10(c9))\n",
    "    \n",
    "UNet_conv = ConvUnet()\n",
    "UNet_conv.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS, LR, BATCH_SIZE, optimizer, scheduler, criterion = get_hyperparameters(UNet_conv)\n",
    "dataloaders, dataset_sizes = get_data_loader(path_train_images, path_train_masks, path_val_images, path_val_masks, BATCH_SIZE)\n",
    "\n",
    "trained_unet_conv, train_losses, val_losses = train(UNet_conv, optimizer, criterion, dataloaders, N_EPOCHS, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('./model'):\n",
    "    os.mkdir('model')\n",
    "torch.save(trained_unet_conv.state_dict(), './model/trained_unetconv.pt')\n",
    "\n",
    "if not os.path.isdir('./figures'):\n",
    "    os.mkdir('figures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training(N_EPOCHS+1, train_losses, val_losses, title='conv_unet_training.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![figures/conv_unet_training.png](figures/conv_unet_training.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions(trained_unet_conv, dataloaders['val'], 'validation_predictions_unet_conv.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![figures/validation_predictions_unet_conv.png](figures/validation_predictions_unet_conv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Replacing the MaxPool layers with convolutional layers makes the model a bit **slower to learn**, however it is able to learn. <br>\n",
    "> It was quite predictable because MaxPool and convolution both select the same samples of the features maps (stride 2). <br>\n",
    "> The MaxPool operation is not learnt, hence the model doesn't have to optimize any weight which is probably the reason why training is slower. <br>\n",
    "> The MaxPool operation is non linear, therefore a convolutional layer (which applies a linear weighted sum of inputs) cannot learn to do it. The optimal learnt convolutional kernel is probably an average of the weights, which mimic the max operation. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 2. As explained in the notes, the encoder loses spatial information. <br>\n",
    "The latent space contains 8x8 features maps, wherehas the initial image size is 128x128. <br>\n",
    "The 4 skip connections gradually add some spatial information to the decoded latent space. <br>\n",
    "If we remove the skip connections, the decoder won't have any spatial information to recontruct the white zones. <br>\n",
    "It will probably be harder for the model to learn. <br>\n",
    "\n",
    "> The following section removes skip connections. The size of the feature maps are not increased by the skip connection. <br>\n",
    "> Hence, the convolutional layers of the features map don't decrease the size of the feature maps (in_channels = out_channels) <br>\n",
    "> The no skip UNet is implemented in  [UNet_no_skip.py](UNet_no_skip.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from UNet_no_skip import UNet_no_skip\n",
    "\n",
    "UNet_no_skip = UNet_no_skip()\n",
    "UNet_no_skip.to(device)\n",
    "\n",
    "N_EPOCHS, LR, BATCH_SIZE, optimizer, scheduler, criterion = get_hyperparameters(UNet_no_skip)\n",
    "dataloaders, dataset_sizes = get_data_loader(path_train_images, path_train_masks, path_val_images, path_val_masks, BATCH_SIZE)\n",
    "\n",
    "trained_unet_no_skip, train_losses, val_losses = train(UNet_no_skip, optimizer, criterion, dataloaders, N_EPOCHS, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(trained_unet_no_skip.state_dict(), './model/trained_unetnoskip.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training(N_EPOCHS+1, train_losses, val_losses, title='no_skip_unet_training.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![figures/no_skip_unet_training.png](figures/no_skip_unet_training.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions(trained_unet_no_skip, dataloaders['val'], 'validation_predictions_unet_no_skip.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![figures/validation_predictions_unet_no_skip.png](figures/validation_predictions_unet_no_skip.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Without skip connections the model is able to learn, however the final validation loss is around 5% higher than the one with the skip connections. <br>\n",
    "> Training is faster without skip connections: it takes around 75 epochs, compared to around 150 for the orginal model. <br>\n",
    "> This is because without concatenations, the decoder features maps have less channels (2x less), hence the decoder convolutional layers have less parameters to optimize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Alternative skip connection <br>\n",
    ">> Addition <br>\n",
    ">> The AddUnet is implemented in [UNet_add.py](UNet_add.py). <br>\n",
    ">> The decoder convolutional layers keep the number channels, and the transpose convolutional layers divide the number of channels by two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from UNet_add import UNet_add\n",
    "    \n",
    "UNet_add = UNet_add()\n",
    "UNet_add.to(device)\n",
    "\n",
    "N_EPOCHS, LR, BATCH_SIZE, optimizer, scheduler, criterion = get_hyperparameters(UNet_add)\n",
    "dataloaders, dataset_sizes = get_data_loader(path_train_images, path_train_masks, path_val_images, path_val_masks, BATCH_SIZE)\n",
    "\n",
    "trained_unet_add, train_losses, val_losses = train(UNet_add, optimizer, criterion, dataloaders, N_EPOCHS, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(trained_unet_add.state_dict(), './model/trained_unetadd.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training(N_EPOCHS+1, train_losses, val_losses, title='add_unet_training.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![figures/add_unet_training.png](figures/add_unet_training.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions(trained_unet_add, dataloaders['val'], 'validation_predictions_unet_add.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![figures/validation_predictions_unet_add.png](figures/validation_predictions_unet_add.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Max <br>\n",
    "The MaxUnet is implemented in [MaxUnet.py](MaxUnet.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MaxUnet import MaxUnet\n",
    "    \n",
    "MaxUnet = MaxUnet()\n",
    "MaxUnet.to(device)\n",
    "\n",
    "N_EPOCHS, LR, BATCH_SIZE, optimizer, scheduler, criterion = get_hyperparameters(MaxUnet)\n",
    "dataloaders, dataset_sizes = get_data_loader(path_train_images, path_train_masks, path_val_images, path_val_masks, BATCH_SIZE)\n",
    "\n",
    "trained_MaxUnet, train_losses, val_losses = train(MaxUnet, optimizer, criterion, dataloaders, N_EPOCHS, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(trained_MaxUnet.state_dict(), './model/trained_unetmax.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training(146, train_losses, val_losses, title='max_unet_training.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![figures/max_unet_training.png](figures/max_unet_training.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions(trained_MaxUnet, dataloaders['val'], 'validation_predictions_unet_max.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![figures/validation_predictions_unet_max.png](figures/validation_predictions_unet_max.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The resuts of MaxUnet are very close to the original UNet. <br>\n",
    "> The validation loss crosses the training loss around epoch 130, like in the original UNet. <br>\n",
    "> It seems that taking the max has the same effect as adding additional feature maps. <br>\n",
    "> It can be linked to the MaxPool operations being replaced by Convolutions in question 2: here we use max of feature maps instead of convolutions on combined feature maps. Like in question 2, results (regarding the loss) are very similar, because convolutions and max can yield similar values, especially since the inputs are normalized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 FCN\n",
    "> The Fully Convolutional neural Network is implemented in [FCN.py](FCN.py). <br>\n",
    "> All double convolution layers were kept. <br>\n",
    "> Remaining shrinking or upsampling layers (MaxPool, TransposeConv) were dismissed. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FCN import FCN\n",
    "    \n",
    "FCN = FCN()\n",
    "FCN.to(device)\n",
    "\n",
    "N_EPOCHS, LR, BATCH_SIZE, optimizer, scheduler, criterion = get_hyperparameters(FCN)\n",
    "dataloaders, dataset_sizes = get_data_loader(path_train_images, path_train_masks, path_val_images, path_val_masks, BATCH_SIZE)\n",
    "\n",
    "trained_fcn, train_losses, val_losses = train(FCN, optimizer, criterion, dataloaders, N_EPOCHS, scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> First of all, a FCN which keeps the image size constant takes much more disk space than the auto-encoder architecture during training: up from 2.5GB to 12GB of GPU RAM. <br>\n",
    "> As the feature map dimensions remain constant (intial image size) and the channels are the same as with the auto-encoder, there are many more computations being carried out by the network, and many more gradients computations to save during training. <br>\n",
    "> The benefit of this method is the **explainability**: we can check feature maps at each stage of the network, contrary to the auto-encoder architecture which shrinks the feature maps. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(trained_fcn.state_dict(), './model/trained_fcn.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training(N_EPOCHS+1, train_losses, val_losses, title='fcn_training.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![figures/fcn_training.png](figures/fcn_training.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions(trained_fcn, dataloaders['val'], 'validation_predictions_fcn.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![figures/validation_predictions_fcn.png](figures/validation_predictions_fcn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4 Threshold for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A classical metric that optimizes both precision and recall in our case is the Intersection over Union (IoU).   \n",
    "> To determine the optimal threshold, we can simply make a grid search over the thresholds, and choose the one that gets the best IoU on the validation set. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNet = UNet()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "UNet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNet.load_state_dict(torch.load('./model/trained_unet_1.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "dataloaders, dataset_sizes = get_data_loader(path_train_images, path_train_masks, path_val_images, path_val_masks, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_IoU(masks, outputs):\n",
    "    \"\"\" Return IoU of masks and outputs.\n",
    "    \n",
    "    When union is 0, it returns 1.\n",
    "    Since intersection C union, I = 0 if union = 0. \n",
    "    In that case the output is correct that's why 1 is returned.\n",
    "    \"\"\"\n",
    "    intersection = torch.sum(masks & outputs, dim=(1,2,3))\n",
    "    union = torch.sum(masks | outputs, dim=(1,2,3))\n",
    "    IoU = torch.where(union == 0, torch.tensor(1), intersection / union)\n",
    "    return IoU.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precision_recall(masks, outputs):\n",
    "    TP = torch.sum(torch.where((masks == 1) & (outputs == 1), torch.tensor(1), torch.tensor(0)), dim=(1,2,3))\n",
    "    FP = torch.sum(torch.where((masks == 0) & (outputs == 1), torch.tensor(1), torch.tensor(0)), dim=(1,2,3))\n",
    "    FN = torch.sum(torch.where((masks == 1) & (outputs == 0), torch.tensor(1), torch.tensor(0)), dim=(1,2,3))\n",
    "    precision = torch.where(TP+FP == 0, torch.tensor(1), TP/(TP+FP)).sum()\n",
    "    recall = torch.where(TP+FN == 0, torch.tensor(1), TP/(TP+FN)).sum()\n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "list_IoU = []\n",
    "list_precision = []\n",
    "list_recall = []\n",
    "thresholds = [i/100 for i in range(30, 91)]\n",
    "with torch.no_grad():    \n",
    "    for threshold in thresholds:\n",
    "        IoU = 0\n",
    "        precision = 0 \n",
    "        recall = 0 \n",
    "        for inputs, masks in dataloaders['val']:\n",
    "            masks = masks.to(device).bool()\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = UNet(inputs) \n",
    "            outputs = torch.where(outputs > threshold, 1., 0.).bool()\n",
    "            IoU += compute_IoU(masks, outputs)\n",
    "            p, r = get_precision_recall(masks, outputs)\n",
    "            precision += p \n",
    "            recall += r \n",
    "        list_IoU.append(IoU / dataset_sizes['val'])\n",
    "        list_precision.append(precision / dataset_sizes['val'] )\n",
    "        list_recall.append(recall / dataset_sizes['val'] )\n",
    "        print(f't = {threshold}, IoU = {list_IoU[-1]:.3f}, precision = {list_precision[-1]:.3f}, recall = {list_recall[-1]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_precision = [x.item() for x in list_precision]\n",
    "list_recall = [x.item() for x in list_recall]\n",
    "list_IoU = [x.item() for x in list_IoU]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(thresholds, list_precision, label='Precision')\n",
    "plt.plot(thresholds, list_recall, label='Recall')\n",
    "plt.plot(thresholds, list_IoU, label='IoU')\n",
    "\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Precision, Recall, and IoU vs. Threshold')\n",
    "plt.legend()\n",
    "\n",
    "plt.grid(True)\n",
    "plt.savefig('IoU_plot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![figures/IoU_plot.png](figures/IoU_plot.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_index = max((x,i) for i,x in enumerate(list_IoU))[1]\n",
    "best_IoU, best_precision, best_recall = list_IoU[best_index], list_precision[best_index], list_recall[best_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Best IoU: {best_IoU:.3f}\\nPrecision for best IoU: {best_precision:.3f}\\nRecall for best IoU: {best_recall:.3f}\\nThreshold: {thresholds[best_index]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> With this method, the best threshold is 0.69. <br>\n",
    "> A few predictions with this threshold can be made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, masks = next(iter(dataloaders['val']))\n",
    "masks = masks.to(device)\n",
    "masks_rgb = masks.repeat(1, 3, 1, 1)\n",
    "inputs = inputs.to(device)\n",
    "outputs = UNet(inputs)\n",
    "n = 5\n",
    "\n",
    "outputs_binary = torch.where(outputs > thresholds[best_index], 1., 0.).repeat(1, 3, 1, 1)\n",
    "outputs = outputs.repeat(1, 3, 1, 1)\n",
    "# Concatenate the images along the width dimension to create a single tensor\n",
    "combined_images = torch.cat([inputs[0:n,:,:,:], masks_rgb[0:n,:,:,:], outputs[0:n,:,:,:], outputs_binary[0:n,:,:,:]], dim=3)\n",
    "# Make a grid with the combined images\n",
    "grid_image = torchvision.utils.make_grid(combined_images, nrow=1, padding=10, normalize=True)\n",
    "# Convert the grid tensor to a numpy array for visualization\n",
    "grid_image_np = grid_image.permute(1, 2, 0).cpu().numpy()\n",
    "# Display the grid of images\n",
    "plt.figure(figsize=(10, 15))\n",
    "plt.imshow(grid_image_np)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "plt.savefig(f'figures/best_threshold_outputs.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![figures/best_threshold_outputs.png](figures/best_threshold_outputs.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
